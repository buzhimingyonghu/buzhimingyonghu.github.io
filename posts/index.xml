<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Victor的世界</title>
        <link>https://buzhimingyonghu.github.io/posts/</link>
        <description>Recent content in Posts on Victor的世界</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh</language>
        <lastBuildDate>Tue, 18 Feb 2025 19:42:02 +0800</lastBuildDate>
        <atom:link href="https://buzhimingyonghu.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>PacificA 一致性测试用例说明</title>
            <link>https://buzhimingyonghu.github.io/posts/zh/pika/pacifica-test-cases/</link>
            <pubDate>Tue, 18 Feb 2025 19:42:02 +0800</pubDate>
            
            <guid>https://buzhimingyonghu.github.io/posts/zh/pika/pacifica-test-cases/</guid>
            <description>&lt;h2 id=&#34;测试用例1基础一致性测试&#34;&gt;测试用例1：基础一致性测试&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;：验证主从复制的基本功能和数据一致性
&lt;strong&gt;步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;向主节点写入数据&lt;/li&gt;
&lt;li&gt;验证两个从节点的数据同步情况&lt;/li&gt;
&lt;li&gt;检查所有节点的复制状态
&lt;strong&gt;预期结果&lt;/strong&gt;：所有节点数据完全一致，复制状态正常&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;测试用例2并发写入一致性测试&#34;&gt;测试用例2：并发写入一致性测试&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;：确保并发写入时的数据一致性
&lt;strong&gt;步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;向主节点并发写入10条数据&lt;/li&gt;
&lt;li&gt;等待数据同步完成&lt;/li&gt;
&lt;li&gt;验证两个从节点的所有数据
&lt;strong&gt;预期结果&lt;/strong&gt;：所有并发写入的数据都正确同步到从节点&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;测试用例3网络分区恢复测试&#34;&gt;测试用例3：网络分区恢复测试&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;：测试网络分区后的一致性恢复
&lt;strong&gt;步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;写入初始数据&lt;/li&gt;
&lt;li&gt;断开从节点1连接（模拟网络分区）&lt;/li&gt;
&lt;li&gt;向主节点写入新数据&lt;/li&gt;
&lt;li&gt;恢复从节点1连接&lt;/li&gt;
&lt;li&gt;验证数据一致性
&lt;strong&gt;预期结果&lt;/strong&gt;：网络恢复后，断开的从节点应同步所有错过的数据&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;测试用例4动态节点添加测试&#34;&gt;测试用例4：动态节点添加测试&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;：验证新增节点时的数据一致性
&lt;strong&gt;步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始只启动主节点和一个从节点&lt;/li&gt;
&lt;li&gt;写入一批初始数据&lt;/li&gt;
&lt;li&gt;添加第二个从节点&lt;/li&gt;
&lt;li&gt;写入新的数据&lt;/li&gt;
&lt;li&gt;验证新旧数据的一致性
&lt;strong&gt;预期结果&lt;/strong&gt;：新加入的从节点应正确接收所有历史数据和新数据&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;测试用例5节点故障恢复测试&#34;&gt;测试用例5：节点故障恢复测试&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;：测试节点故障和恢复时的系统行为
&lt;strong&gt;步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;向所有节点写入初始数据&lt;/li&gt;
&lt;li&gt;模拟从节点1故障&lt;/li&gt;
&lt;li&gt;故障期间写入数据&lt;/li&gt;
&lt;li&gt;恢复从节点1&lt;/li&gt;
&lt;li&gt;写入新的数据&lt;/li&gt;
&lt;li&gt;验证所有数据集
&lt;strong&gt;预期结果&lt;/strong&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;节点故障期间系统继续正常运行&lt;/li&gt;
&lt;li&gt;故障节点恢复后能同步所有错过的数据&lt;/li&gt;
&lt;li&gt;所有节点最终达到数据一致&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;测试环境&#34;&gt;测试环境&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1个主节点（端口：9301）&lt;/li&gt;
&lt;li&gt;2个从节点（端口：9302，9303）&lt;/li&gt;
&lt;li&gt;启用强一致性模式&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;注意事项&#34;&gt;注意事项&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;每个测试用例都包含足够的等待时间，确保数据同步完成&lt;/li&gt;
&lt;li&gt;所有测试都在强一致性模式下进行&lt;/li&gt;
&lt;li&gt;测试过程中会验证数据的完整性和一致性&lt;/li&gt;
&lt;/ol&gt;</description>
            <content type="html"><![CDATA[<h2 id="测试用例1基础一致性测试">测试用例1：基础一致性测试</h2>
<p><strong>目的</strong>：验证主从复制的基本功能和数据一致性
<strong>步骤</strong>：</p>
<ol>
<li>向主节点写入数据</li>
<li>验证两个从节点的数据同步情况</li>
<li>检查所有节点的复制状态
<strong>预期结果</strong>：所有节点数据完全一致，复制状态正常</li>
</ol>
<h2 id="测试用例2并发写入一致性测试">测试用例2：并发写入一致性测试</h2>
<p><strong>目的</strong>：确保并发写入时的数据一致性
<strong>步骤</strong>：</p>
<ol>
<li>向主节点并发写入10条数据</li>
<li>等待数据同步完成</li>
<li>验证两个从节点的所有数据
<strong>预期结果</strong>：所有并发写入的数据都正确同步到从节点</li>
</ol>
<h2 id="测试用例3网络分区恢复测试">测试用例3：网络分区恢复测试</h2>
<p><strong>目的</strong>：测试网络分区后的一致性恢复
<strong>步骤</strong>：</p>
<ol>
<li>写入初始数据</li>
<li>断开从节点1连接（模拟网络分区）</li>
<li>向主节点写入新数据</li>
<li>恢复从节点1连接</li>
<li>验证数据一致性
<strong>预期结果</strong>：网络恢复后，断开的从节点应同步所有错过的数据</li>
</ol>
<h2 id="测试用例4动态节点添加测试">测试用例4：动态节点添加测试</h2>
<p><strong>目的</strong>：验证新增节点时的数据一致性
<strong>步骤</strong>：</p>
<ol>
<li>初始只启动主节点和一个从节点</li>
<li>写入一批初始数据</li>
<li>添加第二个从节点</li>
<li>写入新的数据</li>
<li>验证新旧数据的一致性
<strong>预期结果</strong>：新加入的从节点应正确接收所有历史数据和新数据</li>
</ol>
<h2 id="测试用例5节点故障恢复测试">测试用例5：节点故障恢复测试</h2>
<p><strong>目的</strong>：测试节点故障和恢复时的系统行为
<strong>步骤</strong>：</p>
<ol>
<li>向所有节点写入初始数据</li>
<li>模拟从节点1故障</li>
<li>故障期间写入数据</li>
<li>恢复从节点1</li>
<li>写入新的数据</li>
<li>验证所有数据集
<strong>预期结果</strong>：</li>
</ol>
<ul>
<li>节点故障期间系统继续正常运行</li>
<li>故障节点恢复后能同步所有错过的数据</li>
<li>所有节点最终达到数据一致</li>
</ul>
<h2 id="测试环境">测试环境</h2>
<ul>
<li>1个主节点（端口：9301）</li>
<li>2个从节点（端口：9302，9303）</li>
<li>启用强一致性模式</li>
</ul>
<h2 id="注意事项">注意事项</h2>
<ol>
<li>每个测试用例都包含足够的等待时间，确保数据同步完成</li>
<li>所有测试都在强一致性模式下进行</li>
<li>测试过程中会验证数据的完整性和一致性</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>PacificA解读</title>
            <link>https://buzhimingyonghu.github.io/posts/zh/distributed/pacifica-test-cases/</link>
            <pubDate>Tue, 18 Feb 2025 19:42:02 +0800</pubDate>
            
            <guid>https://buzhimingyonghu.github.io/posts/zh/distributed/pacifica-test-cases/</guid>
            <description>&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;p&gt;大规模分布式存储因数据量增长而广受关注，复制机制是实现高可用性和高吞吐的关键。尽管共识研究为复制协议奠定基础，但架构设计和工程实现仍具挑战。本文分享了PacificA协议基于日志的存储系统设计复制机制的经验，提出一种简单、实用、强一致的通用复制框架，展示其支持多种设计选择的灵活性。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;1-pacifica-流程&#34;&gt;1. PacificA 流程&lt;/h1&gt;
&lt;p&gt;系统通过主从模式实现数据复制，每份数据由一个副本组负责，组内指定主服务器，其余为备份，配置变化由版本号跟踪。本文关注强一致性复制协议，确保分布式系统行为与单机一致（线性一致性）。&lt;/p&gt;
&lt;h2 id=&#34;11-主从复制&#34;&gt;1.1 主从复制&lt;/h2&gt;
&lt;p&gt;我们将客户端请求分为两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;读数据的查询请求&lt;/li&gt;
&lt;li&gt;写数据的更新请求。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果复制组中的所有服务器以相同顺序处理相同的请求集（假设更新是确定性的），则可以实现强一致性。因此，主服务器为更新分配连续且单调递增的序列号，并指示所有备服务器按此顺序连续处理请求。&lt;/p&gt;
&lt;h3 id=&#34;正常情况下的处理流程&#34;&gt;正常情况下的处理流程：&lt;/h3&gt;
&lt;h4 id=&#34;读请求的处理&#34;&gt;读请求的处理：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;当主服务器接收到读请求时，它直接使用当前提交列表中（未实现）记录的状态来处理请求。查询请求不影响数据的一致性，因此主服务器可以立刻返回结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;写请求的处理&#34;&gt;写请求的处理：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;主服务器会为写请求分配一个递增的全局序列号，确保所有请求按照固定的顺序处理。&lt;/li&gt;
&lt;li&gt;主服务器会将包含配置版本（未实现）和序列号的请求和CommittedID，通过一个 &lt;code&gt;prepare&lt;/code&gt; 消息发送给所有从服务器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;从服务器的处理&#34;&gt;从服务器的处理：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;每个备服务器在收到 &lt;code&gt;prepare&lt;/code&gt; 消息后，会按照序列号顺序将请求添加到自己的准备列表中，将请求标记为“已准备”。&lt;/li&gt;
&lt;li&gt;随后，备服务器向主服务器发送一个 &lt;code&gt;prepared&lt;/code&gt; 消息作为确认。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;提交到状态机&#34;&gt;提交到状态机：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;当主服务器收到所有从服务器的确认后，才会将该请求标记为已提交。此时，主服务器更新它的提交点，使其指向已提交的最高序列号位置。&lt;/li&gt;
&lt;li&gt;主服务器会向客户端返回确认消息，表示请求已成功完成。&lt;/li&gt;
&lt;li&gt;在每次发送 &lt;code&gt;prepare&lt;/code&gt; 消息时，主服务器还会附带当前提交点的序列号，告知备服务器哪些请求已经提交。这样，备服务器可以将自己的提交点前移，与主服务器保持一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;一致性保证&#34;&gt;一致性保证：&lt;/h3&gt;
&lt;p&gt;主服务器仅在所有从服务器将请求添加到准备列表后，才会将其加入提交列表，确保提交列表与备服务器的准备列表一致且包含于其中。同时，从服务器仅在主服务器标记请求为提交后，才会将其视为已提交，确保备服务器的提交列表始终包含于主服务器的提交范围内。&lt;/p&gt;
&lt;h3 id=&#34;提交-invariant&#34;&gt;提交 Invariant：&lt;/h3&gt;
&lt;p&gt;形成了“提交 Invariant”，即对于主服务器 &lt;code&gt;p&lt;/code&gt; 和任何备服务器 &lt;code&gt;q&lt;/code&gt;，始终有：&lt;/p&gt;
&lt;p&gt;committedq ⊆ committedp ⊆ preparedq&lt;/p&gt;
&lt;p&gt;这保证了主备之间的数据一致性和同步。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;12-配置管理&#34;&gt;1.2 配置管理&lt;/h2&gt;
&lt;h3 id=&#34;设计一个全局配置管理器&#34;&gt;设计一个全局配置管理器：&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;负责管理和维护系统中所有副本组的配置。&lt;/li&gt;
&lt;li&gt;对于每个副本组，配置管理器会保存当前的配置和配置版本。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;全局配置管理器的功能&#34;&gt;全局配置管理器的功能：&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;重新配置&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;检测副本是否出现故障，决定是否移除副本，或者重启副本配置。&lt;/li&gt;
&lt;li&gt;添加新的副本。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;增添从节点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据设定的规则决定是否添加新配置。&lt;/li&gt;
&lt;li&gt;配置规则：是否版本匹配，检测副本的 &lt;code&gt;committedID&lt;/code&gt; 是否匹配（是否存在，是否小于主的 &lt;code&gt;committedID&lt;/code&gt;）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;主崩溃后，重新配置&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果发生网络分区，导致主服务器与副本之间断开连接，可能会出现冲突的重新配置请求。例如，主服务器可能希望移除一些副本，而某些副本则希望移除主服务器。&lt;/li&gt;
&lt;li&gt;依旧先检测是否匹配规则 &lt;code&gt;L&lt;/code&gt;，匹配成功后配置管理器接受的请求会“胜出”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;故障检测和主服务器不变性&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主服务器不变性要求，在任何时刻，服务器 &lt;code&gt;p&lt;/code&gt; 只有在配置管理器认为它是当前配置中的主服务器时，才会将自己视为主服务器。这样可以确保在系统中，副本组中最多只有一台服务器会认为自己是主服务器。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结：&lt;/h3&gt;
&lt;p&gt;配置管理器负责协调和维护系统的配置，确保副本组的配置一致性、版本控制和故障恢复。主服务器不变性确保在系统中始终只有一个主服务器，而不会发生多个服务器同时作为主服务器的情况。&lt;/p&gt;</description>
            <content type="html"><![CDATA[<h2 id="摘要">摘要</h2>
<p>大规模分布式存储因数据量增长而广受关注，复制机制是实现高可用性和高吞吐的关键。尽管共识研究为复制协议奠定基础，但架构设计和工程实现仍具挑战。本文分享了PacificA协议基于日志的存储系统设计复制机制的经验，提出一种简单、实用、强一致的通用复制框架，展示其支持多种设计选择的灵活性。</p>
<hr>
<h1 id="1-pacifica-流程">1. PacificA 流程</h1>
<p>系统通过主从模式实现数据复制，每份数据由一个副本组负责，组内指定主服务器，其余为备份，配置变化由版本号跟踪。本文关注强一致性复制协议，确保分布式系统行为与单机一致（线性一致性）。</p>
<h2 id="11-主从复制">1.1 主从复制</h2>
<p>我们将客户端请求分为两类：</p>
<ol>
<li>读数据的查询请求</li>
<li>写数据的更新请求。</li>
</ol>
<p>如果复制组中的所有服务器以相同顺序处理相同的请求集（假设更新是确定性的），则可以实现强一致性。因此，主服务器为更新分配连续且单调递增的序列号，并指示所有备服务器按此顺序连续处理请求。</p>
<h3 id="正常情况下的处理流程">正常情况下的处理流程：</h3>
<h4 id="读请求的处理">读请求的处理：</h4>
<ul>
<li>当主服务器接收到读请求时，它直接使用当前提交列表中（未实现）记录的状态来处理请求。查询请求不影响数据的一致性，因此主服务器可以立刻返回结果。</li>
</ul>
<h4 id="写请求的处理">写请求的处理：</h4>
<ul>
<li>主服务器会为写请求分配一个递增的全局序列号，确保所有请求按照固定的顺序处理。</li>
<li>主服务器会将包含配置版本（未实现）和序列号的请求和CommittedID，通过一个 <code>prepare</code> 消息发送给所有从服务器。</li>
</ul>
<h4 id="从服务器的处理">从服务器的处理：</h4>
<ul>
<li>每个备服务器在收到 <code>prepare</code> 消息后，会按照序列号顺序将请求添加到自己的准备列表中，将请求标记为“已准备”。</li>
<li>随后，备服务器向主服务器发送一个 <code>prepared</code> 消息作为确认。</li>
</ul>
<h4 id="提交到状态机">提交到状态机：</h4>
<ul>
<li>当主服务器收到所有从服务器的确认后，才会将该请求标记为已提交。此时，主服务器更新它的提交点，使其指向已提交的最高序列号位置。</li>
<li>主服务器会向客户端返回确认消息，表示请求已成功完成。</li>
<li>在每次发送 <code>prepare</code> 消息时，主服务器还会附带当前提交点的序列号，告知备服务器哪些请求已经提交。这样，备服务器可以将自己的提交点前移，与主服务器保持一致。</li>
</ul>
<h3 id="一致性保证">一致性保证：</h3>
<p>主服务器仅在所有从服务器将请求添加到准备列表后，才会将其加入提交列表，确保提交列表与备服务器的准备列表一致且包含于其中。同时，从服务器仅在主服务器标记请求为提交后，才会将其视为已提交，确保备服务器的提交列表始终包含于主服务器的提交范围内。</p>
<h3 id="提交-invariant">提交 Invariant：</h3>
<p>形成了“提交 Invariant”，即对于主服务器 <code>p</code> 和任何备服务器 <code>q</code>，始终有：</p>
<p>committedq ⊆ committedp ⊆ preparedq</p>
<p>这保证了主备之间的数据一致性和同步。</p>
<hr>
<h2 id="12-配置管理">1.2 配置管理</h2>
<h3 id="设计一个全局配置管理器">设计一个全局配置管理器：</h3>
<ul>
<li>负责管理和维护系统中所有副本组的配置。</li>
<li>对于每个副本组，配置管理器会保存当前的配置和配置版本。</li>
</ul>
<h3 id="全局配置管理器的功能">全局配置管理器的功能：</h3>
<ol>
<li>
<p><strong>重新配置</strong>：</p>
<ul>
<li>检测副本是否出现故障，决定是否移除副本，或者重启副本配置。</li>
<li>添加新的副本。</li>
</ul>
</li>
<li>
<p><strong>增添从节点</strong>：</p>
<ul>
<li>根据设定的规则决定是否添加新配置。</li>
<li>配置规则：是否版本匹配，检测副本的 <code>committedID</code> 是否匹配（是否存在，是否小于主的 <code>committedID</code>）。</li>
</ul>
</li>
<li>
<p><strong>主崩溃后，重新配置</strong>：</p>
<ul>
<li>如果发生网络分区，导致主服务器与副本之间断开连接，可能会出现冲突的重新配置请求。例如，主服务器可能希望移除一些副本，而某些副本则希望移除主服务器。</li>
<li>依旧先检测是否匹配规则 <code>L</code>，匹配成功后配置管理器接受的请求会“胜出”。</li>
</ul>
</li>
<li>
<p><strong>故障检测和主服务器不变性</strong>：</p>
<ul>
<li>主服务器不变性要求，在任何时刻，服务器 <code>p</code> 只有在配置管理器认为它是当前配置中的主服务器时，才会将自己视为主服务器。这样可以确保在系统中，副本组中最多只有一台服务器会认为自己是主服务器。</li>
</ul>
</li>
</ol>
<h3 id="总结">总结：</h3>
<p>配置管理器负责协调和维护系统的配置，确保副本组的配置一致性、版本控制和故障恢复。主服务器不变性确保在系统中始终只有一个主服务器，而不会发生多个服务器同时作为主服务器的情况。</p>
<hr>
<h2 id="13-协调状态">1.3 协调状态</h2>
<h3 id="主服务器变更后如何保证数据的一致性"><strong>主服务器变更后，如何保证数据的一致性</strong></h3>
<p>如果主服务器已故障，则会触发主服务器变更流程，副本成为新的主服务器。新的主服务器在处理新请求前，需要完成协调过程，也就是处理 <code>preparedID</code> 和 <code>committedID</code>。</p>
<h4 id="协调过程"><strong>协调过程</strong></h4>
<ol>
<li>
<p><strong>最初状态</strong>：</p>
<ul>
<li><code>A</code> 是主服务器，<code>B</code>、<code>C</code> 和 <code>D</code> 是副本。</li>
<li><code>committedB</code> 是 <code>committedA</code> 的子集，<code>committedA</code> 又是任何副本的 <code>prepared</code> 的子集。</li>
</ul>
</li>
<li>
<p><strong>协调过程</strong>：</p>
<ul>
<li>假设发生了重新配置，将 <code>B</code> 替代故障的 <code>A</code> 成为主服务器。</li>
<li><code>B</code> 完成协调后，新的 <code>committedB</code> 与旧的 <code>preparedB</code> 相同，也就是说，现在所有副本的 <code>prepared</code> 和 <code>preparedB</code> 对齐。</li>
</ul>
</li>
</ol>
<h4 id="总结-1"><strong>总结</strong></h4>
<ol>
<li>新主会提交现在所有的已准备的日志，使得 <code>preparedIdB</code> 和 <code>committedIdB</code> 相同。</li>
<li>使所有的副本 <code>preparedID</code> 和主的 <code>preparedID</code> 看齐，多余的删除，少的补上去。</li>
</ol>
<h2 id="14-添加新的副本">1.4 添加新的副本</h2>
<p>在副本组中的某些副本发生故障后，为了恢复冗余级别，可以将新的副本添加到副本组中。在添加新服务器到配置中时，必须保持提交 Invariant，新副本在加入副本组前，必须先拥有完整的准备列表，这样可以确保一致性。</p>
<h3 id="同步方案">同步方案：</h3>
<ol>
<li><strong>简单同步方案</strong>
<ul>
<li>主服务器暂停处理新的更新，等待新副本从现有副本复制好准备列表后再继续工作。这虽然保证了一致性，但可能会拖慢系统处理速度。</li>
</ul>
</li>
<li><strong>候选副本机制</strong>
<ul>
<li>候选副本：主服务器不停止处理更新，而是将新副本作为“候选副本”加入系统。主服务器会把更新的准备消息发送给候选副本。</li>
<li>确认机制：候选副本收到并确认消息后，才能正式成为系统中的一员。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="15-配置管理器的可用性和性能">1.5 配置管理器的可用性和性能</h2>
<h3 id="配置管理器的作用">配置管理器的作用：</h3>
<ul>
<li>负责管理系统所有副本组的当前配置，简化管理。</li>
<li>与数据复制协议分离，提高系统容错能力，可容忍最多 <code>n-1</code> 个副本故障。</li>
</ul>
<h3 id="配置管理器的高可用性">配置管理器的高可用性：</h3>
<ul>
<li>采用 <strong>复制状态机</strong> 结合 <strong>Paxos 协议</strong>，确保系统一致性和容错能力。</li>
<li>部署多个实例（通常为 5 或 7 台服务器），可容忍少数服务器故障。</li>
</ul>
<hr>
<h2 id="16-主从模式与-paxos-协议的比较">1.6 主/从模式与 Paxos 协议的比较</h2>
<table>
  <thead>
      <tr>
          <th>对比项</th>
          <th>主从模式</th>
          <th>Paxos 协议</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>一致性</strong></td>
          <td>需所有副本准备完成</td>
          <td>需多数副本准备完成</td>
      </tr>
      <tr>
          <td><strong>容错性</strong></td>
          <td>单点故障影响较大</td>
          <td>容忍少数副本故障</td>
      </tr>
      <tr>
          <td><strong>重新配置</strong></td>
          <td>配置管理器协助，简单</td>
          <td>需共识决策，较复杂</td>
      </tr>
      <tr>
          <td><strong>适用场景</strong></td>
          <td>结构简单，工程易实现</td>
          <td>适用于更高容错需求</td>
      </tr>
  </tbody>
</table>
<p>在实践中，主从模式因其简单性常被选用，而 Paxos 适用于更高可靠性需求的系统。本文选择主从模式，以简化实现并保证一致性。</p>
<h2 id="2-pacifica-复制框架">2. PacificA 复制框架</h2>
<p>在分布式日志型存储系统中，复制框架通常用于确保数据的持久性和高效存储，特别是当数据分布在多个服务器上时。这个设计通过日志记录、检查点、内存缓存和磁盘映像的组合来提供高效的数据管理，避免了频繁的磁盘随机写入。以下是设计的主要步骤和原理：</p>
<h3 id="1-日志记录确保持久性">1. 日志记录（确保持久性）</h3>
<ul>
<li>系统在接收到更新请求时，首先将更新写入日志中。这一步确保即使系统故障，日志也能作为持久化的备份来恢复数据。</li>
</ul>
<h3 id="2-内存数据结构更新">2. 内存数据结构更新</h3>
<ul>
<li>记录到日志后，系统会将更新应用到内存中的数据结构中，以便在内存中快速处理和查询最新的数据。</li>
</ul>
<h3 id="3-定期创建检查点">3. 定期创建检查点</h3>
<ul>
<li>为了防止内存溢出，系统会定期在磁盘上创建检查点，保存内存中的数据快照。这一步将内存中的所有数据写入磁盘，形成一个持久化的检查点。</li>
</ul>
<h3 id="4-日志截断">4. 日志截断</h3>
<ul>
<li>创建检查点后，日志中已经包含在检查点中的更新可以被截断或删除，因为它们已经安全地存储在磁盘上。这一步优化了日志的存储需求，防止日志无限增长。</li>
</ul>
<h3 id="5-查询处理">5. 查询处理</h3>
<ul>
<li>查询通过内存中的数据结构、检查点以及磁盘映像三者结合来完成。这样可以优先从内存中获取最新数据，若数据不在内存中，则进一步查找检查点和最终的磁盘映像。</li>
</ul>
<h3 id="设计优势">设计优势</h3>
<ul>
<li><strong>顺序写入</strong>：基于日志的设计将更新转换为顺序写入，避免了磁盘上的随机写入，从而大大提升了写入速度和系统性能。</li>
<li><strong>快速恢复</strong>：通过日志记录和检查点，系统在故障后可以快速重放日志和检查点，从而恢复数据。</li>
<li><strong>存储优化</strong>：日志截断避免了日志文件无限增长，确保持久化存储的效率。</li>
</ul>
<h2 id="逻辑复制">逻辑复制</h2>
<p>逻辑复制是一种数据复制方式，主要用于在分布式系统中实现数据一致性。它强调逻辑上保持所有副本的数据状态一致，即每个副本的状态逻辑上应与主副本一致，虽然副本的物理存储方式可以有所不同。以下是具体内容的解释：</p>
<h3 id="1-状态的一致性">1. 状态的一致性</h3>
<ul>
<li>在逻辑复制中，所有副本都逻辑上保持相同的状态，并能够处理相同类型的更新和查询。每个副本可以自行决定何时进行检查点（保存快照）或合并，但它们的状态在逻辑上是一致的。</li>
</ul>
<h3 id="2-已准备列表和应用日志">2. 已准备列表和应用日志</h3>
<ul>
<li><strong>已准备列表</strong>：用于存储那些已接收但尚未正式提交的更新。这些请求已经准备好，但可能尚未应用到主存储状态中。</li>
<li><strong>应用日志</strong>：用于存储所有收到的客户端请求的日志记录，包括已提交和未提交的请求。为避免写入开销，这两个列表可以合并，已准备的更新只要保存在应用日志中即可。</li>
<li><strong>日志条目</strong> 包含三个关键字段：配置版本号、序列号和最后的提交序列号。这些信息帮助系统追踪请求状态，尤其是在主节点更改时确保日志的唯一性和一致性。</li>
</ul>
<h3 id="3-两阶段流程">3. 两阶段流程</h3>
<ul>
<li><strong>第一阶段</strong>：当副本收到请求消息（包括请求内容、版本号、序列号等信息）时，消息会被追加到副本的应用日志中。</li>
<li><strong>第二阶段</strong>：当请求被正式提交后，会直接应用到内存中，而无需再次写入日志，因为它已经记录在日志中。</li>
</ul>
<h3 id="4-日志截断-1">4. 日志截断</h3>
<ul>
<li>日志中的已提交更新在生成检查点后会被截断，只保留未提交的更新。这减少了日志文件的大小，同时保留系统故障恢复时所需的内容。</li>
</ul>
<h3 id="5-检查点与恢复">5. 检查点与恢复</h3>
<ul>
<li>每个检查点保存了某个序列号范围内的所有更新，帮助副本在故障后进行恢复。</li>
<li>副本的磁盘映像（持久化存储）与其最后的序列号关联，可以在副本重启时作为恢复的起点。</li>
</ul>
<h2 id="逻辑复制的变体逻辑-v">逻辑复制的变体（逻辑-V）</h2>
<p>逻辑复制中的一个特殊变体是<strong>逻辑-V</strong>，它通过减少资源消耗来优化系统性能：</p>
<h3 id="1-仅主节点处理更新">1. 仅主节点处理更新</h3>
<ul>
<li>在逻辑-V中，只有主节点执行状态更新、生成检查点和合并操作。</li>
<li>次节点只负责记录更新日志，而不真正应用更新，从而减少了次节点的内存和 CPU 使用。</li>
</ul>
<h3 id="2-次节点检查点传输">2. 次节点检查点传输</h3>
<ul>
<li>次节点无需自行生成检查点，而是直接从主节点获取已完成的检查点。这种方式减少了副本所需的内存和 CPU 开销，但增加了网络负载，因为检查点需要通过网络传输。</li>
</ul>
<h3 id="3-故障转移的影响">3. 故障转移的影响</h3>
<ul>
<li>在逻辑-V中，若主节点发生故障，次节点需要重放日志以恢复到最新状态，再生成检查点。这导致逻辑-V 下的故障转移时间会更长。</li>
</ul>
<h2 id="优缺点对比">优缺点对比</h2>
<table>
  <thead>
      <tr>
          <th>方案</th>
          <th>优势</th>
          <th>缺点</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>逻辑复制（标准模式）</strong></td>
          <td>提供更高的本地一致性和独立性，每个副本都可以独立生成检查点</td>
          <td>每个副本消耗更多的资源（内存、CPU），增加了系统开销</td>
      </tr>
      <tr>
          <td><strong>逻辑-V</strong></td>
          <td>减少了次节点的资源消耗，使系统更具扩展性</td>
          <td>增加了网络负载和故障转移的延迟，次节点成为主节点的过程中需要更多的恢复时间</td>
      </tr>
  </tbody>
</table>
<h2 id="总结-2">总结</h2>
<p>逻辑复制和逻辑-V 都是为了在分布式系统中实现高效的副本一致性。</p>
<ul>
<li><strong>逻辑复制</strong> 适合资源较充足、希望副本独立性的场景。</li>
<li><strong>逻辑-V</strong> 适合注重资源节省、网络带宽充足但允许更高故障恢复时间的场景。</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>基于PacificA协议Pika主从一致性</title>
            <link>https://buzhimingyonghu.github.io/posts/zh/pika/pacifica-consistency/</link>
            <pubDate>Tue, 18 Feb 2025 19:42:02 +0800</pubDate>
            
            <guid>https://buzhimingyonghu.github.io/posts/zh/pika/pacifica-consistency/</guid>
            <description>&lt;h1 id=&#34;pacifica-协议概述&#34;&gt;PacificA 协议概述&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2008/02/tr-2008-25.pdf&#34;&gt;PacificA &lt;/a&gt;协议简单来说分为两部分：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- 数据复制&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;配置管理&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;由于在 Pika 中，配置管理主要由 &lt;code&gt;pika_sentinel&lt;/code&gt; 负责，本文主要关注通过主从模式的数据复制及其与 &lt;code&gt;pika_sentinel&lt;/code&gt; 配合的协调过程。&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;在-pika-中的应用&#34;&gt;在 Pika 中的应用&lt;/h2&gt;
&lt;p&gt;主要分为三个部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PacificA 中主从模式的数据一致流程处理&lt;/li&gt;
&lt;li&gt;分布式日志型存储系统的设计&lt;/li&gt;
&lt;li&gt;故障恢复后的协调状态&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;启动-pacifica&#34;&gt;启动 PacificA&lt;/h2&gt;
&lt;p&gt;在 Pika 中，建立普通主从连接的命令为：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;slaveof &amp;lt;ip&amp;gt; &amp;lt;port&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果需要启动 PacificA 协议，需要增加 strong 参数：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;slaveof &amp;lt;ip&amp;gt; &amp;lt;port&amp;gt; strong
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当从节点执行上述命令时，会触发 slaveofcmd，读取相关参数，并由 pika_server 保存这些信息，随后异步交由 PikaAuxiliaryThread 线程（以下简称 PAT）处理。
PAT 是 PacificA 协议中的核心辅助线程，负责：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- 状态机状态切换
- 主从之间的心跳发送及超时检查
- 主从之间的同步任务
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;pacifica-主从模式的数据一致流程&#34;&gt;PacificA 主从模式的数据一致流程&lt;/h2&gt;
&lt;h3 id=&#34;主从建立连接的四个阶段&#34;&gt;主从建立连接的四个阶段&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;MetaSync：主从元数据的同步和检查&lt;/li&gt;
&lt;li&gt;TrySync：判断数据完整性，选择全量同步或增量同步&lt;/li&gt;
&lt;li&gt;Candidate：从节点作为候选者，追加完整的准备列表&lt;/li&gt;
&lt;li&gt;BinlogSync：正式加入集群，开始进行数据复制&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/11268449-19db-4d14-af3b-0aebd9e54a54&#34; alt=&#34;image&#34;&gt;
下面是基本的数据结构：
&lt;img src=&#34;https://github.com/user-attachments/assets/d81d704d-34ce-4c8e-aaff-d4f137a45035&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;metasync-阶段&#34;&gt;MetaSync 阶段&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/19e287da-0630-4381-b09e-75527ea76a20&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;从节点的 PAT 线程通过发送 MetaReq 请求与主节点建立连接，其中包含 is_consistency 字段，表示强一致性请求。
主节点收到请求后，若 consistency 标记为 true，则会：&lt;/p&gt;</description>
            <content type="html"><![CDATA[<h1 id="pacifica-协议概述">PacificA 协议概述</h1>
<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2008/02/tr-2008-25.pdf">PacificA </a>协议简单来说分为两部分：</p>
<p><strong>- 数据复制</strong></p>
<ol start="2">
<li>配置管理</li>
</ol>
<p><em>由于在 Pika 中，配置管理主要由 <code>pika_sentinel</code> 负责，本文主要关注通过主从模式的数据复制及其与 <code>pika_sentinel</code> 配合的协调过程。</em></p>
<h2 id="在-pika-中的应用">在 Pika 中的应用</h2>
<p>主要分为三个部分：</p>
<ol>
<li>PacificA 中主从模式的数据一致流程处理</li>
<li>分布式日志型存储系统的设计</li>
<li>故障恢复后的协调状态</li>
</ol>
<hr>
<h2 id="启动-pacifica">启动 PacificA</h2>
<p>在 Pika 中，建立普通主从连接的命令为：</p>
<pre tabindex="0"><code>slaveof &lt;ip&gt; &lt;port&gt;
</code></pre><p>如果需要启动 PacificA 协议，需要增加 strong 参数：</p>
<pre tabindex="0"><code>slaveof &lt;ip&gt; &lt;port&gt; strong
</code></pre><p>当从节点执行上述命令时，会触发 slaveofcmd，读取相关参数，并由 pika_server 保存这些信息，随后异步交由 PikaAuxiliaryThread 线程（以下简称 PAT）处理。
PAT 是 PacificA 协议中的核心辅助线程，负责：</p>
<pre><code>- 状态机状态切换
- 主从之间的心跳发送及超时检查
- 主从之间的同步任务
</code></pre>
<h2 id="pacifica-主从模式的数据一致流程">PacificA 主从模式的数据一致流程</h2>
<h3 id="主从建立连接的四个阶段">主从建立连接的四个阶段</h3>
<ol>
<li>MetaSync：主从元数据的同步和检查</li>
<li>TrySync：判断数据完整性，选择全量同步或增量同步</li>
<li>Candidate：从节点作为候选者，追加完整的准备列表</li>
<li>BinlogSync：正式加入集群，开始进行数据复制</li>
</ol>
<p><img src="https://github.com/user-attachments/assets/11268449-19db-4d14-af3b-0aebd9e54a54" alt="image">
下面是基本的数据结构：
<img src="https://github.com/user-attachments/assets/d81d704d-34ce-4c8e-aaff-d4f137a45035" alt="image"></p>
<h2 id="metasync-阶段">MetaSync 阶段</h2>
<p><img src="https://github.com/user-attachments/assets/19e287da-0630-4381-b09e-75527ea76a20" alt="image"></p>
<p>从节点的 PAT 线程通过发送 MetaReq 请求与主节点建立连接，其中包含 is_consistency 字段，表示强一致性请求。
主节点收到请求后，若 consistency 标记为 true，则会：</p>
<ol>
<li>设置所有数据库的 consistency 标记</li>
<li>初始化上下文</li>
<li>判断是否需要进入协调状态</li>
</ol>
<p><img src="https://github.com/user-attachments/assets/0f54ef7b-661e-45de-96a9-d794ebb3840b" alt="image"></p>
<p>随后，从节点收到主节点返回的 MetaSyncRes，并执行以下操作：</p>
<ol>
<li>
<p>比较本地和主节点的数据库结构 (db_structs) 是否一致</p>
</li>
<li>
<p>如果本地 replication_id 与主节点不一致，且本地 replication_id 为空，则执行全量同步；否则进行增量同步</p>
</li>
<li>
<p>根据同步类型更新从节点的状态：</p>
<ul>
<li>全量同步：设置状态为 kTryDBSync</li>
<li>增量同步：设置状态为 kTryConnect</li>
</ul>
</li>
</ol>
<h2 id="trysync-阶段">TrySync 阶段</h2>
<p><img src="https://github.com/user-attachments/assets/1e0b3450-e91f-4aeb-909f-1f6a4d6e4855" alt="image"></p>
<p>全量同步完成后，从节点更新自身的 committedID 和 preparedID，并发送 TrySyncReq 请求，携带 committedID 确认日志一致性。
主节点验证后，返回包含主节点 preparedID 的 TrySyncRes，从节点将 preparedID 与主节点对齐，完成增量同步。</p>
<p>流程总结：</p>
<ol>
<li>
<p>从节点发送 TrySyncReq，带有 committedID。</p>
</li>
<li>
<p>主节点检查 committedID：</p>
<ul>
<li>若主节点 committedID 大于从节点，表示同步正常。</li>
<li>若从节点 committedID 大于主节点，表示选主失败。</li>
</ul>
</li>
<li>
<p>主节点返回 TrySyncRes，包含主节点的 preparedID，从节点需对齐。</p>
</li>
</ol>
<h2 id="binlogsync-阶段">BinlogSync 阶段</h2>
<p><img src="https://github.com/user-attachments/assets/44cf483f-102b-4f85-b79e-c786a8e60b2c" alt="image"></p>
<p>主节点收到从节点的第一次 binlog 请求后，将从节点设置为候选者状态，并追加日志。
主节点通过心跳包和 binlog 数据通知从节点，将日志分阶段写入本地：</p>
<ul>
<li>从节点收到 binlogSync 后，先写入 binlog，等待主节点通知哪些请求已提交。</li>
<li>主节点收到所有从节点确认后，将请求标记为已提交，更新提交点，确保与所有从节点保持一致。</li>
</ul>
<h2 id="分布式日志型存储系统的设计">分布式日志型存储系统的设计</h2>
<p>PacificA 中采用逻辑复制的方式，具体包括：</p>
<ol>
<li>状态的一致性：所有副本逻辑上保持相同的状态，并可处理相同类型的更新和查询。</li>
<li>日志记录：系统在接收到更新请求时，首先将其写入日志中，确保即使系统故障也能通过日志恢复数据。</li>
<li>内存数据结构更新：日志记录完成后，将更新应用到内存中的数据结构中。</li>
<li>定期创建检查点：防止内存溢出，定期将数据快照保存到磁盘，形成持久化检查点。</li>
<li>日志截断：检查点创建后，删除已存储到检查点的日志，优化存储需求。</li>
</ol>
<p><img src="https://github.com/user-attachments/assets/dcc12756-9217-4ba5-b5e3-36ac0d91283e" alt="image"></p>
<h2 id="故障恢复后的协调状态">故障恢复后的协调状态</h2>
<p><img src="https://github.com/user-attachments/assets/1e6fb58b-dd02-48c3-bf11-52ae9e57f6d9" alt="image"></p>
<h3 id="初始状态">初始状态</h3>
<ul>
<li>A 是主节点，B、C 和 D 是副本节点。</li>
<li>committedB 是 committedA 的子集，committedA 是所有副本 prepared 的子集。</li>
</ul>
<h3 id="故障恢复">故障恢复</h3>
<h3 id="当主节点-a-故障时">当主节点 A 故障时：</h3>
<ol>
<li>系统重新配置，将 B 提升为新主节点。</li>
<li>B 完成协调后，新的 committedB 与旧的 preparedB 保持一致，所有副本的 preparedID 与新主节点的 preparedID 对齐。</li>
</ol>
<h2 id="一次写请求的操作流程">一次写请求的操作流程</h2>
<ol>
<li>写 binlog：等待所有从节点追加日志后，执行写入数据库请求。</li>
<li>主节点处理 binlog 请求：
<ul>
<li>若非一致性模式，按传统主从复制执行。</li>
<li>若为一致性模式：</li>
</ul>
</li>
<li>coordinator_ 追加日志并记录 offset。</li>
<li>等待从节点同步，更新主节点的 committedID。</li>
<li>若同步失败（超时 10 秒），记录信息并退出。</li>
</ol>
<p><img src="https://github.com/user-attachments/assets/41df6eae-f144-47c4-b554-c0bcf512b62a" alt="image"></p>
]]></content>
        </item>
        
    </channel>
</rss>
