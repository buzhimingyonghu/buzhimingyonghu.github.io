<!DOCTYPE html>
<html lang="zh">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="摘要 大规模分布式存储因数据量增长而广受关注，复制机制是实现高可用性和高吞吐的关键。尽管共识研究为复制协议奠定基础，但架构设计和工程实现仍具挑战。本文分享了PacificA协议基于日志的存储系统设计复制机制的经验，提出一种简单、实用、强一致的通用复制框架，展示其支持多种设计选择的灵活性。
1. PacificA 流程 系统通过主从模式实现数据复制，每份数据由一个副本组负责，组内指定主服务器，其余为备份，配置变化由版本号跟踪。本文关注强一致性复制协议，确保分布式系统行为与单机一致（线性一致性）。
1.1 主从复制 我们将客户端请求分为两类：
读数据的查询请求 写数据的更新请求。 如果复制组中的所有服务器以相同顺序处理相同的请求集（假设更新是确定性的），则可以实现强一致性。因此，主服务器为更新分配连续且单调递增的序列号，并指示所有备服务器按此顺序连续处理请求。
正常情况下的处理流程： 读请求的处理： 当主服务器接收到读请求时，它直接使用当前提交列表中（未实现）记录的状态来处理请求。查询请求不影响数据的一致性，因此主服务器可以立刻返回结果。 写请求的处理： 主服务器会为写请求分配一个递增的全局序列号，确保所有请求按照固定的顺序处理。 主服务器会将包含配置版本（未实现）和序列号的请求和CommittedID，通过一个 prepare 消息发送给所有从服务器。 从服务器的处理： 每个备服务器在收到 prepare 消息后，会按照序列号顺序将请求添加到自己的准备列表中，将请求标记为“已准备”。 随后，备服务器向主服务器发送一个 prepared 消息作为确认。 提交到状态机： 当主服务器收到所有从服务器的确认后，才会将该请求标记为已提交。此时，主服务器更新它的提交点，使其指向已提交的最高序列号位置。 主服务器会向客户端返回确认消息，表示请求已成功完成。 在每次发送 prepare 消息时，主服务器还会附带当前提交点的序列号，告知备服务器哪些请求已经提交。这样，备服务器可以将自己的提交点前移，与主服务器保持一致。 一致性保证： 主服务器仅在所有从服务器将请求添加到准备列表后，才会将其加入提交列表，确保提交列表与备服务器的准备列表一致且包含于其中。同时，从服务器仅在主服务器标记请求为提交后，才会将其视为已提交，确保备服务器的提交列表始终包含于主服务器的提交范围内。
提交 Invariant： 形成了“提交 Invariant”，即对于主服务器 p 和任何备服务器 q，始终有：
committedq ⊆ committedp ⊆ preparedq
这保证了主备之间的数据一致性和同步。
1.2 配置管理 设计一个全局配置管理器： 负责管理和维护系统中所有副本组的配置。 对于每个副本组，配置管理器会保存当前的配置和配置版本。 全局配置管理器的功能： 重新配置：
检测副本是否出现故障，决定是否移除副本，或者重启副本配置。 添加新的副本。 增添从节点：
根据设定的规则决定是否添加新配置。 配置规则：是否版本匹配，检测副本的 committedID 是否匹配（是否存在，是否小于主的 committedID）。 主崩溃后，重新配置：
如果发生网络分区，导致主服务器与副本之间断开连接，可能会出现冲突的重新配置请求。例如，主服务器可能希望移除一些副本，而某些副本则希望移除主服务器。 依旧先检测是否匹配规则 L，匹配成功后配置管理器接受的请求会“胜出”。 故障检测和主服务器不变性：
主服务器不变性要求，在任何时刻，服务器 p 只有在配置管理器认为它是当前配置中的主服务器时，才会将自己视为主服务器。这样可以确保在系统中，副本组中最多只有一台服务器会认为自己是主服务器。 总结： 配置管理器负责协调和维护系统的配置，确保副本组的配置一致性、版本控制和故障恢复。主服务器不变性确保在系统中始终只有一个主服务器，而不会发生多个服务器同时作为主服务器的情况。
" />
<meta name="keywords" content="Victor, 博客, 技术, 生活, PacificA, 分布式, 一致性" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="//localhost:1313/posts/zh/distributed/pacifica-test-cases/" />


    <title>
        
            PacificA解读 :: Victor的世界 
        
    </title>





  <link rel="stylesheet" href="/main.min.244183cde1a38e0b08f82c11791181288f9aac1cc9618cd6f4e9e7710c5768ba.css" integrity="sha256-JEGDzeGjjgsI&#43;CwReRGBKI&#43;arBzJYYzW9OnncQxXaLo=" crossorigin="anonymous">





    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="PacificA解读">
  <meta itemprop="description" content="摘要 大规模分布式存储因数据量增长而广受关注，复制机制是实现高可用性和高吞吐的关键。尽管共识研究为复制协议奠定基础，但架构设计和工程实现仍具挑战。本文分享了PacificA协议基于日志的存储系统设计复制机制的经验，提出一种简单、实用、强一致的通用复制框架，展示其支持多种设计选择的灵活性。
1. PacificA 流程 系统通过主从模式实现数据复制，每份数据由一个副本组负责，组内指定主服务器，其余为备份，配置变化由版本号跟踪。本文关注强一致性复制协议，确保分布式系统行为与单机一致（线性一致性）。
1.1 主从复制 我们将客户端请求分为两类：
读数据的查询请求 写数据的更新请求。 如果复制组中的所有服务器以相同顺序处理相同的请求集（假设更新是确定性的），则可以实现强一致性。因此，主服务器为更新分配连续且单调递增的序列号，并指示所有备服务器按此顺序连续处理请求。
正常情况下的处理流程： 读请求的处理： 当主服务器接收到读请求时，它直接使用当前提交列表中（未实现）记录的状态来处理请求。查询请求不影响数据的一致性，因此主服务器可以立刻返回结果。 写请求的处理： 主服务器会为写请求分配一个递增的全局序列号，确保所有请求按照固定的顺序处理。 主服务器会将包含配置版本（未实现）和序列号的请求和CommittedID，通过一个 prepare 消息发送给所有从服务器。 从服务器的处理： 每个备服务器在收到 prepare 消息后，会按照序列号顺序将请求添加到自己的准备列表中，将请求标记为“已准备”。 随后，备服务器向主服务器发送一个 prepared 消息作为确认。 提交到状态机： 当主服务器收到所有从服务器的确认后，才会将该请求标记为已提交。此时，主服务器更新它的提交点，使其指向已提交的最高序列号位置。 主服务器会向客户端返回确认消息，表示请求已成功完成。 在每次发送 prepare 消息时，主服务器还会附带当前提交点的序列号，告知备服务器哪些请求已经提交。这样，备服务器可以将自己的提交点前移，与主服务器保持一致。 一致性保证： 主服务器仅在所有从服务器将请求添加到准备列表后，才会将其加入提交列表，确保提交列表与备服务器的准备列表一致且包含于其中。同时，从服务器仅在主服务器标记请求为提交后，才会将其视为已提交，确保备服务器的提交列表始终包含于主服务器的提交范围内。
提交 Invariant： 形成了“提交 Invariant”，即对于主服务器 p 和任何备服务器 q，始终有：
committedq ⊆ committedp ⊆ preparedq
这保证了主备之间的数据一致性和同步。
1.2 配置管理 设计一个全局配置管理器： 负责管理和维护系统中所有副本组的配置。 对于每个副本组，配置管理器会保存当前的配置和配置版本。 全局配置管理器的功能： 重新配置：
检测副本是否出现故障，决定是否移除副本，或者重启副本配置。 添加新的副本。 增添从节点：
根据设定的规则决定是否添加新配置。 配置规则：是否版本匹配，检测副本的 committedID 是否匹配（是否存在，是否小于主的 committedID）。 主崩溃后，重新配置：
如果发生网络分区，导致主服务器与副本之间断开连接，可能会出现冲突的重新配置请求。例如，主服务器可能希望移除一些副本，而某些副本则希望移除主服务器。 依旧先检测是否匹配规则 L，匹配成功后配置管理器接受的请求会“胜出”。 故障检测和主服务器不变性：
主服务器不变性要求，在任何时刻，服务器 p 只有在配置管理器认为它是当前配置中的主服务器时，才会将自己视为主服务器。这样可以确保在系统中，副本组中最多只有一台服务器会认为自己是主服务器。 总结： 配置管理器负责协调和维护系统的配置，确保副本组的配置一致性、版本控制和故障恢复。主服务器不变性确保在系统中始终只有一个主服务器，而不会发生多个服务器同时作为主服务器的情况。">
  <meta itemprop="datePublished" content="2025-02-18T19:42:02+08:00">
  <meta itemprop="dateModified" content="2025-02-18T19:42:02+08:00">
  <meta itemprop="wordCount" content="251">
  <meta itemprop="keywords" content="PacificA,分布式,一致性">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="PacificA解读">
  <meta name="twitter:description" content="摘要 大规模分布式存储因数据量增长而广受关注，复制机制是实现高可用性和高吞吐的关键。尽管共识研究为复制协议奠定基础，但架构设计和工程实现仍具挑战。本文分享了PacificA协议基于日志的存储系统设计复制机制的经验，提出一种简单、实用、强一致的通用复制框架，展示其支持多种设计选择的灵活性。
1. PacificA 流程 系统通过主从模式实现数据复制，每份数据由一个副本组负责，组内指定主服务器，其余为备份，配置变化由版本号跟踪。本文关注强一致性复制协议，确保分布式系统行为与单机一致（线性一致性）。
1.1 主从复制 我们将客户端请求分为两类：
读数据的查询请求 写数据的更新请求。 如果复制组中的所有服务器以相同顺序处理相同的请求集（假设更新是确定性的），则可以实现强一致性。因此，主服务器为更新分配连续且单调递增的序列号，并指示所有备服务器按此顺序连续处理请求。
正常情况下的处理流程： 读请求的处理： 当主服务器接收到读请求时，它直接使用当前提交列表中（未实现）记录的状态来处理请求。查询请求不影响数据的一致性，因此主服务器可以立刻返回结果。 写请求的处理： 主服务器会为写请求分配一个递增的全局序列号，确保所有请求按照固定的顺序处理。 主服务器会将包含配置版本（未实现）和序列号的请求和CommittedID，通过一个 prepare 消息发送给所有从服务器。 从服务器的处理： 每个备服务器在收到 prepare 消息后，会按照序列号顺序将请求添加到自己的准备列表中，将请求标记为“已准备”。 随后，备服务器向主服务器发送一个 prepared 消息作为确认。 提交到状态机： 当主服务器收到所有从服务器的确认后，才会将该请求标记为已提交。此时，主服务器更新它的提交点，使其指向已提交的最高序列号位置。 主服务器会向客户端返回确认消息，表示请求已成功完成。 在每次发送 prepare 消息时，主服务器还会附带当前提交点的序列号，告知备服务器哪些请求已经提交。这样，备服务器可以将自己的提交点前移，与主服务器保持一致。 一致性保证： 主服务器仅在所有从服务器将请求添加到准备列表后，才会将其加入提交列表，确保提交列表与备服务器的准备列表一致且包含于其中。同时，从服务器仅在主服务器标记请求为提交后，才会将其视为已提交，确保备服务器的提交列表始终包含于主服务器的提交范围内。
提交 Invariant： 形成了“提交 Invariant”，即对于主服务器 p 和任何备服务器 q，始终有：
committedq ⊆ committedp ⊆ preparedq
这保证了主备之间的数据一致性和同步。
1.2 配置管理 设计一个全局配置管理器： 负责管理和维护系统中所有副本组的配置。 对于每个副本组，配置管理器会保存当前的配置和配置版本。 全局配置管理器的功能： 重新配置：
检测副本是否出现故障，决定是否移除副本，或者重启副本配置。 添加新的副本。 增添从节点：
根据设定的规则决定是否添加新配置。 配置规则：是否版本匹配，检测副本的 committedID 是否匹配（是否存在，是否小于主的 committedID）。 主崩溃后，重新配置：
如果发生网络分区，导致主服务器与副本之间断开连接，可能会出现冲突的重新配置请求。例如，主服务器可能希望移除一些副本，而某些副本则希望移除主服务器。 依旧先检测是否匹配规则 L，匹配成功后配置管理器接受的请求会“胜出”。 故障检测和主服务器不变性：
主服务器不变性要求，在任何时刻，服务器 p 只有在配置管理器认为它是当前配置中的主服务器时，才会将自己视为主服务器。这样可以确保在系统中，副本组中最多只有一台服务器会认为自己是主服务器。 总结： 配置管理器负责协调和维护系统的配置，确保副本组的配置一致性、版本控制和故障恢复。主服务器不变性确保在系统中始终只有一个主服务器，而不会发生多个服务器同时作为主服务器的情况。">



    <meta property="og:url" content="//localhost:1313/posts/zh/distributed/pacifica-test-cases/">
  <meta property="og:site_name" content="Victor的世界">
  <meta property="og:title" content="PacificA解读">
  <meta property="og:description" content="摘要 大规模分布式存储因数据量增长而广受关注，复制机制是实现高可用性和高吞吐的关键。尽管共识研究为复制协议奠定基础，但架构设计和工程实现仍具挑战。本文分享了PacificA协议基于日志的存储系统设计复制机制的经验，提出一种简单、实用、强一致的通用复制框架，展示其支持多种设计选择的灵活性。
1. PacificA 流程 系统通过主从模式实现数据复制，每份数据由一个副本组负责，组内指定主服务器，其余为备份，配置变化由版本号跟踪。本文关注强一致性复制协议，确保分布式系统行为与单机一致（线性一致性）。
1.1 主从复制 我们将客户端请求分为两类：
读数据的查询请求 写数据的更新请求。 如果复制组中的所有服务器以相同顺序处理相同的请求集（假设更新是确定性的），则可以实现强一致性。因此，主服务器为更新分配连续且单调递增的序列号，并指示所有备服务器按此顺序连续处理请求。
正常情况下的处理流程： 读请求的处理： 当主服务器接收到读请求时，它直接使用当前提交列表中（未实现）记录的状态来处理请求。查询请求不影响数据的一致性，因此主服务器可以立刻返回结果。 写请求的处理： 主服务器会为写请求分配一个递增的全局序列号，确保所有请求按照固定的顺序处理。 主服务器会将包含配置版本（未实现）和序列号的请求和CommittedID，通过一个 prepare 消息发送给所有从服务器。 从服务器的处理： 每个备服务器在收到 prepare 消息后，会按照序列号顺序将请求添加到自己的准备列表中，将请求标记为“已准备”。 随后，备服务器向主服务器发送一个 prepared 消息作为确认。 提交到状态机： 当主服务器收到所有从服务器的确认后，才会将该请求标记为已提交。此时，主服务器更新它的提交点，使其指向已提交的最高序列号位置。 主服务器会向客户端返回确认消息，表示请求已成功完成。 在每次发送 prepare 消息时，主服务器还会附带当前提交点的序列号，告知备服务器哪些请求已经提交。这样，备服务器可以将自己的提交点前移，与主服务器保持一致。 一致性保证： 主服务器仅在所有从服务器将请求添加到准备列表后，才会将其加入提交列表，确保提交列表与备服务器的准备列表一致且包含于其中。同时，从服务器仅在主服务器标记请求为提交后，才会将其视为已提交，确保备服务器的提交列表始终包含于主服务器的提交范围内。
提交 Invariant： 形成了“提交 Invariant”，即对于主服务器 p 和任何备服务器 q，始终有：
committedq ⊆ committedp ⊆ preparedq
这保证了主备之间的数据一致性和同步。
1.2 配置管理 设计一个全局配置管理器： 负责管理和维护系统中所有副本组的配置。 对于每个副本组，配置管理器会保存当前的配置和配置版本。 全局配置管理器的功能： 重新配置：
检测副本是否出现故障，决定是否移除副本，或者重启副本配置。 添加新的副本。 增添从节点：
根据设定的规则决定是否添加新配置。 配置规则：是否版本匹配，检测副本的 committedID 是否匹配（是否存在，是否小于主的 committedID）。 主崩溃后，重新配置：
如果发生网络分区，导致主服务器与副本之间断开连接，可能会出现冲突的重新配置请求。例如，主服务器可能希望移除一些副本，而某些副本则希望移除主服务器。 依旧先检测是否匹配规则 L，匹配成功后配置管理器接受的请求会“胜出”。 故障检测和主服务器不变性：
主服务器不变性要求，在任何时刻，服务器 p 只有在配置管理器认为它是当前配置中的主服务器时，才会将自己视为主服务器。这样可以确保在系统中，副本组中最多只有一台服务器会认为自己是主服务器。 总结： 配置管理器负责协调和维护系统的配置，确保副本组的配置一致性、版本控制和故障恢复。主服务器不变性确保在系统中始终只有一个主服务器，而不会发生多个服务器同时作为主服务器的情况。">
  <meta property="og:locale" content="zh">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-02-18T19:42:02+08:00">
    <meta property="article:modified_time" content="2025-02-18T19:42:02+08:00">
    <meta property="article:tag" content="PacificA">
    <meta property="article:tag" content="分布式">
    <meta property="article:tag" content="一致性">






    <meta property="article:published_time" content="2025-02-18 19:42:02 &#43;0800 CST" />












    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                Victor的世界</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/about">关于</a></li><li><a href="/posts">博客</a></li><li><a href="https://github.com/buzhimingyonghu">GitHub</a></li>
        <ul class="submenu">
            <li class="dropdown">
                <a href="javascript:void(0)" class="dropbtn">zh</a>
                <div class="dropdown-content">
                        
                </div>
            </li>
        </ul>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        

        
      </p>

      
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder">
          <path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path>
        </svg>
        <a href="/posts/posts">Posts</a>
      </p>
      
    </div>

    <article>
      <h1 class="post-title">
        <a href="//localhost:1313/posts/zh/distributed/pacifica-test-cases/">PacificA解读</a>
      </h1>

      

      

      <div class="post-content">
        <h2 id="摘要">摘要</h2>
<p>大规模分布式存储因数据量增长而广受关注，复制机制是实现高可用性和高吞吐的关键。尽管共识研究为复制协议奠定基础，但架构设计和工程实现仍具挑战。本文分享了PacificA协议基于日志的存储系统设计复制机制的经验，提出一种简单、实用、强一致的通用复制框架，展示其支持多种设计选择的灵活性。</p>
<hr>
<h1 id="1-pacifica-流程">1. PacificA 流程</h1>
<p>系统通过主从模式实现数据复制，每份数据由一个副本组负责，组内指定主服务器，其余为备份，配置变化由版本号跟踪。本文关注强一致性复制协议，确保分布式系统行为与单机一致（线性一致性）。</p>
<h2 id="11-主从复制">1.1 主从复制</h2>
<p>我们将客户端请求分为两类：</p>
<ol>
<li>读数据的查询请求</li>
<li>写数据的更新请求。</li>
</ol>
<p>如果复制组中的所有服务器以相同顺序处理相同的请求集（假设更新是确定性的），则可以实现强一致性。因此，主服务器为更新分配连续且单调递增的序列号，并指示所有备服务器按此顺序连续处理请求。</p>
<h3 id="正常情况下的处理流程">正常情况下的处理流程：</h3>
<h4 id="读请求的处理">读请求的处理：</h4>
<ul>
<li>当主服务器接收到读请求时，它直接使用当前提交列表中（未实现）记录的状态来处理请求。查询请求不影响数据的一致性，因此主服务器可以立刻返回结果。</li>
</ul>
<h4 id="写请求的处理">写请求的处理：</h4>
<ul>
<li>主服务器会为写请求分配一个递增的全局序列号，确保所有请求按照固定的顺序处理。</li>
<li>主服务器会将包含配置版本（未实现）和序列号的请求和CommittedID，通过一个 <code>prepare</code> 消息发送给所有从服务器。</li>
</ul>
<h4 id="从服务器的处理">从服务器的处理：</h4>
<ul>
<li>每个备服务器在收到 <code>prepare</code> 消息后，会按照序列号顺序将请求添加到自己的准备列表中，将请求标记为“已准备”。</li>
<li>随后，备服务器向主服务器发送一个 <code>prepared</code> 消息作为确认。</li>
</ul>
<h4 id="提交到状态机">提交到状态机：</h4>
<ul>
<li>当主服务器收到所有从服务器的确认后，才会将该请求标记为已提交。此时，主服务器更新它的提交点，使其指向已提交的最高序列号位置。</li>
<li>主服务器会向客户端返回确认消息，表示请求已成功完成。</li>
<li>在每次发送 <code>prepare</code> 消息时，主服务器还会附带当前提交点的序列号，告知备服务器哪些请求已经提交。这样，备服务器可以将自己的提交点前移，与主服务器保持一致。</li>
</ul>
<h3 id="一致性保证">一致性保证：</h3>
<p>主服务器仅在所有从服务器将请求添加到准备列表后，才会将其加入提交列表，确保提交列表与备服务器的准备列表一致且包含于其中。同时，从服务器仅在主服务器标记请求为提交后，才会将其视为已提交，确保备服务器的提交列表始终包含于主服务器的提交范围内。</p>
<h3 id="提交-invariant">提交 Invariant：</h3>
<p>形成了“提交 Invariant”，即对于主服务器 <code>p</code> 和任何备服务器 <code>q</code>，始终有：</p>
<p>committedq ⊆ committedp ⊆ preparedq</p>
<p>这保证了主备之间的数据一致性和同步。</p>
<hr>
<h2 id="12-配置管理">1.2 配置管理</h2>
<h3 id="设计一个全局配置管理器">设计一个全局配置管理器：</h3>
<ul>
<li>负责管理和维护系统中所有副本组的配置。</li>
<li>对于每个副本组，配置管理器会保存当前的配置和配置版本。</li>
</ul>
<h3 id="全局配置管理器的功能">全局配置管理器的功能：</h3>
<ol>
<li>
<p><strong>重新配置</strong>：</p>
<ul>
<li>检测副本是否出现故障，决定是否移除副本，或者重启副本配置。</li>
<li>添加新的副本。</li>
</ul>
</li>
<li>
<p><strong>增添从节点</strong>：</p>
<ul>
<li>根据设定的规则决定是否添加新配置。</li>
<li>配置规则：是否版本匹配，检测副本的 <code>committedID</code> 是否匹配（是否存在，是否小于主的 <code>committedID</code>）。</li>
</ul>
</li>
<li>
<p><strong>主崩溃后，重新配置</strong>：</p>
<ul>
<li>如果发生网络分区，导致主服务器与副本之间断开连接，可能会出现冲突的重新配置请求。例如，主服务器可能希望移除一些副本，而某些副本则希望移除主服务器。</li>
<li>依旧先检测是否匹配规则 <code>L</code>，匹配成功后配置管理器接受的请求会“胜出”。</li>
</ul>
</li>
<li>
<p><strong>故障检测和主服务器不变性</strong>：</p>
<ul>
<li>主服务器不变性要求，在任何时刻，服务器 <code>p</code> 只有在配置管理器认为它是当前配置中的主服务器时，才会将自己视为主服务器。这样可以确保在系统中，副本组中最多只有一台服务器会认为自己是主服务器。</li>
</ul>
</li>
</ol>
<h3 id="总结">总结：</h3>
<p>配置管理器负责协调和维护系统的配置，确保副本组的配置一致性、版本控制和故障恢复。主服务器不变性确保在系统中始终只有一个主服务器，而不会发生多个服务器同时作为主服务器的情况。</p>
<hr>
<h2 id="13-协调状态">1.3 协调状态</h2>
<h3 id="主服务器变更后如何保证数据的一致性"><strong>主服务器变更后，如何保证数据的一致性</strong></h3>
<p>如果主服务器已故障，则会触发主服务器变更流程，副本成为新的主服务器。新的主服务器在处理新请求前，需要完成协调过程，也就是处理 <code>preparedID</code> 和 <code>committedID</code>。</p>
<h4 id="协调过程"><strong>协调过程</strong></h4>
<ol>
<li>
<p><strong>最初状态</strong>：</p>
<ul>
<li><code>A</code> 是主服务器，<code>B</code>、<code>C</code> 和 <code>D</code> 是副本。</li>
<li><code>committedB</code> 是 <code>committedA</code> 的子集，<code>committedA</code> 又是任何副本的 <code>prepared</code> 的子集。</li>
</ul>
</li>
<li>
<p><strong>协调过程</strong>：</p>
<ul>
<li>假设发生了重新配置，将 <code>B</code> 替代故障的 <code>A</code> 成为主服务器。</li>
<li><code>B</code> 完成协调后，新的 <code>committedB</code> 与旧的 <code>preparedB</code> 相同，也就是说，现在所有副本的 <code>prepared</code> 和 <code>preparedB</code> 对齐。</li>
</ul>
</li>
</ol>
<h4 id="总结-1"><strong>总结</strong></h4>
<ol>
<li>新主会提交现在所有的已准备的日志，使得 <code>preparedIdB</code> 和 <code>committedIdB</code> 相同。</li>
<li>使所有的副本 <code>preparedID</code> 和主的 <code>preparedID</code> 看齐，多余的删除，少的补上去。</li>
</ol>
<h2 id="14-添加新的副本">1.4 添加新的副本</h2>
<p>在副本组中的某些副本发生故障后，为了恢复冗余级别，可以将新的副本添加到副本组中。在添加新服务器到配置中时，必须保持提交 Invariant，新副本在加入副本组前，必须先拥有完整的准备列表，这样可以确保一致性。</p>
<h3 id="同步方案">同步方案：</h3>
<ol>
<li><strong>简单同步方案</strong>
<ul>
<li>主服务器暂停处理新的更新，等待新副本从现有副本复制好准备列表后再继续工作。这虽然保证了一致性，但可能会拖慢系统处理速度。</li>
</ul>
</li>
<li><strong>候选副本机制</strong>
<ul>
<li>候选副本：主服务器不停止处理更新，而是将新副本作为“候选副本”加入系统。主服务器会把更新的准备消息发送给候选副本。</li>
<li>确认机制：候选副本收到并确认消息后，才能正式成为系统中的一员。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="15-配置管理器的可用性和性能">1.5 配置管理器的可用性和性能</h2>
<h3 id="配置管理器的作用">配置管理器的作用：</h3>
<ul>
<li>负责管理系统所有副本组的当前配置，简化管理。</li>
<li>与数据复制协议分离，提高系统容错能力，可容忍最多 <code>n-1</code> 个副本故障。</li>
</ul>
<h3 id="配置管理器的高可用性">配置管理器的高可用性：</h3>
<ul>
<li>采用 <strong>复制状态机</strong> 结合 <strong>Paxos 协议</strong>，确保系统一致性和容错能力。</li>
<li>部署多个实例（通常为 5 或 7 台服务器），可容忍少数服务器故障。</li>
</ul>
<hr>
<h2 id="16-主从模式与-paxos-协议的比较">1.6 主/从模式与 Paxos 协议的比较</h2>
<table>
  <thead>
      <tr>
          <th>对比项</th>
          <th>主从模式</th>
          <th>Paxos 协议</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>一致性</strong></td>
          <td>需所有副本准备完成</td>
          <td>需多数副本准备完成</td>
      </tr>
      <tr>
          <td><strong>容错性</strong></td>
          <td>单点故障影响较大</td>
          <td>容忍少数副本故障</td>
      </tr>
      <tr>
          <td><strong>重新配置</strong></td>
          <td>配置管理器协助，简单</td>
          <td>需共识决策，较复杂</td>
      </tr>
      <tr>
          <td><strong>适用场景</strong></td>
          <td>结构简单，工程易实现</td>
          <td>适用于更高容错需求</td>
      </tr>
  </tbody>
</table>
<p>在实践中，主从模式因其简单性常被选用，而 Paxos 适用于更高可靠性需求的系统。本文选择主从模式，以简化实现并保证一致性。</p>
<h2 id="2-pacifica-复制框架">2. PacificA 复制框架</h2>
<p>在分布式日志型存储系统中，复制框架通常用于确保数据的持久性和高效存储，特别是当数据分布在多个服务器上时。这个设计通过日志记录、检查点、内存缓存和磁盘映像的组合来提供高效的数据管理，避免了频繁的磁盘随机写入。以下是设计的主要步骤和原理：</p>
<h3 id="1-日志记录确保持久性">1. 日志记录（确保持久性）</h3>
<ul>
<li>系统在接收到更新请求时，首先将更新写入日志中。这一步确保即使系统故障，日志也能作为持久化的备份来恢复数据。</li>
</ul>
<h3 id="2-内存数据结构更新">2. 内存数据结构更新</h3>
<ul>
<li>记录到日志后，系统会将更新应用到内存中的数据结构中，以便在内存中快速处理和查询最新的数据。</li>
</ul>
<h3 id="3-定期创建检查点">3. 定期创建检查点</h3>
<ul>
<li>为了防止内存溢出，系统会定期在磁盘上创建检查点，保存内存中的数据快照。这一步将内存中的所有数据写入磁盘，形成一个持久化的检查点。</li>
</ul>
<h3 id="4-日志截断">4. 日志截断</h3>
<ul>
<li>创建检查点后，日志中已经包含在检查点中的更新可以被截断或删除，因为它们已经安全地存储在磁盘上。这一步优化了日志的存储需求，防止日志无限增长。</li>
</ul>
<h3 id="5-查询处理">5. 查询处理</h3>
<ul>
<li>查询通过内存中的数据结构、检查点以及磁盘映像三者结合来完成。这样可以优先从内存中获取最新数据，若数据不在内存中，则进一步查找检查点和最终的磁盘映像。</li>
</ul>
<h3 id="设计优势">设计优势</h3>
<ul>
<li><strong>顺序写入</strong>：基于日志的设计将更新转换为顺序写入，避免了磁盘上的随机写入，从而大大提升了写入速度和系统性能。</li>
<li><strong>快速恢复</strong>：通过日志记录和检查点，系统在故障后可以快速重放日志和检查点，从而恢复数据。</li>
<li><strong>存储优化</strong>：日志截断避免了日志文件无限增长，确保持久化存储的效率。</li>
</ul>
<h2 id="逻辑复制">逻辑复制</h2>
<p>逻辑复制是一种数据复制方式，主要用于在分布式系统中实现数据一致性。它强调逻辑上保持所有副本的数据状态一致，即每个副本的状态逻辑上应与主副本一致，虽然副本的物理存储方式可以有所不同。以下是具体内容的解释：</p>
<h3 id="1-状态的一致性">1. 状态的一致性</h3>
<ul>
<li>在逻辑复制中，所有副本都逻辑上保持相同的状态，并能够处理相同类型的更新和查询。每个副本可以自行决定何时进行检查点（保存快照）或合并，但它们的状态在逻辑上是一致的。</li>
</ul>
<h3 id="2-已准备列表和应用日志">2. 已准备列表和应用日志</h3>
<ul>
<li><strong>已准备列表</strong>：用于存储那些已接收但尚未正式提交的更新。这些请求已经准备好，但可能尚未应用到主存储状态中。</li>
<li><strong>应用日志</strong>：用于存储所有收到的客户端请求的日志记录，包括已提交和未提交的请求。为避免写入开销，这两个列表可以合并，已准备的更新只要保存在应用日志中即可。</li>
<li><strong>日志条目</strong> 包含三个关键字段：配置版本号、序列号和最后的提交序列号。这些信息帮助系统追踪请求状态，尤其是在主节点更改时确保日志的唯一性和一致性。</li>
</ul>
<h3 id="3-两阶段流程">3. 两阶段流程</h3>
<ul>
<li><strong>第一阶段</strong>：当副本收到请求消息（包括请求内容、版本号、序列号等信息）时，消息会被追加到副本的应用日志中。</li>
<li><strong>第二阶段</strong>：当请求被正式提交后，会直接应用到内存中，而无需再次写入日志，因为它已经记录在日志中。</li>
</ul>
<h3 id="4-日志截断-1">4. 日志截断</h3>
<ul>
<li>日志中的已提交更新在生成检查点后会被截断，只保留未提交的更新。这减少了日志文件的大小，同时保留系统故障恢复时所需的内容。</li>
</ul>
<h3 id="5-检查点与恢复">5. 检查点与恢复</h3>
<ul>
<li>每个检查点保存了某个序列号范围内的所有更新，帮助副本在故障后进行恢复。</li>
<li>副本的磁盘映像（持久化存储）与其最后的序列号关联，可以在副本重启时作为恢复的起点。</li>
</ul>
<h2 id="逻辑复制的变体逻辑-v">逻辑复制的变体（逻辑-V）</h2>
<p>逻辑复制中的一个特殊变体是<strong>逻辑-V</strong>，它通过减少资源消耗来优化系统性能：</p>
<h3 id="1-仅主节点处理更新">1. 仅主节点处理更新</h3>
<ul>
<li>在逻辑-V中，只有主节点执行状态更新、生成检查点和合并操作。</li>
<li>次节点只负责记录更新日志，而不真正应用更新，从而减少了次节点的内存和 CPU 使用。</li>
</ul>
<h3 id="2-次节点检查点传输">2. 次节点检查点传输</h3>
<ul>
<li>次节点无需自行生成检查点，而是直接从主节点获取已完成的检查点。这种方式减少了副本所需的内存和 CPU 开销，但增加了网络负载，因为检查点需要通过网络传输。</li>
</ul>
<h3 id="3-故障转移的影响">3. 故障转移的影响</h3>
<ul>
<li>在逻辑-V中，若主节点发生故障，次节点需要重放日志以恢复到最新状态，再生成检查点。这导致逻辑-V 下的故障转移时间会更长。</li>
</ul>
<h2 id="优缺点对比">优缺点对比</h2>
<table>
  <thead>
      <tr>
          <th>方案</th>
          <th>优势</th>
          <th>缺点</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>逻辑复制（标准模式）</strong></td>
          <td>提供更高的本地一致性和独立性，每个副本都可以独立生成检查点</td>
          <td>每个副本消耗更多的资源（内存、CPU），增加了系统开销</td>
      </tr>
      <tr>
          <td><strong>逻辑-V</strong></td>
          <td>减少了次节点的资源消耗，使系统更具扩展性</td>
          <td>增加了网络负载和故障转移的延迟，次节点成为主节点的过程中需要更多的恢复时间</td>
      </tr>
  </tbody>
</table>
<h2 id="总结-2">总结</h2>
<p>逻辑复制和逻辑-V 都是为了在分布式系统中实现高效的副本一致性。</p>
<ul>
<li><strong>逻辑复制</strong> 适合资源较充足、希望副本独立性的场景。</li>
<li><strong>逻辑-V</strong> 适合注重资源节省、网络带宽充足但允许更高故障恢复时间的场景。</li>
</ul>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="//localhost:1313/tags/pacifica/">PacificA</a></span>
        <span class="tag"><a href="//localhost:1313/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a></span>
        <span class="tag"><a href="//localhost:1313/tags/%E4%B8%80%E8%87%B4%E6%80%A7/">一致性</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2025-02-18 19:42
        
      </p>
    </div>

    
    <div class="pagination">
        

        <div class="pagination__buttons">
            

            
            <span class="button next">
                <a href="//localhost:1313/posts/zh/pika/pacifica-test-cases/">
                    <span class="button__text">PacificA 一致性测试用例说明</span>
                    <span class="button__icon">→</span>
                </a>
            </span>
            
        </div>
    </div>


  </main>

            </div>

            
                <footer class="footer">
    
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.e89fda0f29b95d33f6f4224dd9e5cf69d84aff3818be2b0d73e731689cc374261b016d17d46f8381962fb4a1577ba3017b1f23509d894f6e66431f988c00889e.js" integrity="sha512-6J/aDym5XTP29CJN2eXPadhK/zgYvisNc&#43;cxaJzDdCYbAW0X1G&#43;DgZYvtKFXe6MBex8jUJ2JT25mQx&#43;YjACIng=="></script>




    </body>
</html>
